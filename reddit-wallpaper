#!/usr/bin/env python3
"""
NAME

    reddit-wallpaper

SYNOPSIS

    reddit-wallpaper [-h --help] [SUBREDDITS]

DESCRIPTION

    Download top images from subreddits and save them in a folder to be used as background images. Caches based off of image url MD5 hash.

EXAMPLES

    reddit-wallpaper CityPorn:top:10:week EarthPorn:new

AUTHOR

    Alex Wendland <me@alexwendland.com>
    Rick Harris <rconradharris@gmail.com>
"""
import configparser
import argparse
import collections
import json
import os
import re
import shutil
import sys
import urllib.request, urllib.error, urllib.parse
import urllib.parse
import hashlib


CONFIG_PATH = "~/.reddit-wallpaper.conf"
DEFAULT_DOWNLOAD_DIRECTORY = "~/reddit-wallpaper"
RE_RESOLUTION_IN_TITLE = re.compile(".*\[\s*(\d+)\s*[xX]\s*(\d+)\s*\].*")
DEFAULT_MIN_RESOLUTION = '1024x768'
IMAGE_EXTENSIONS = set(['jpg', 'jpeg', 'png'])

VERBOSE = False


def set_verbose(verbose):
    global VERBOSE
    VERBOSE = verbose


def log(msg):
    if VERBOSE:
        print(msg, file=sys.stderr)


def _urlopen(url):
    user_agent = "Mozilla/5.0 (X11; U; Linux i686) Gecko/20071127 Firefox/2.0.0.11" 
    opener = urllib.request.build_opener()
    opener.addheaders = [('User-Agent', user_agent)]
    return opener.open(url)


def _get_extension_from_url(url):
    url_path = urllib.parse.urlparse(url).path
    parts = url_path.rsplit('.', 1)
    try:
        return parts[1]
    except IndexError:
        return None

def _download_image(dirname, image_url, existing_images):
    ext = _get_extension_from_url(image_url)
    assert ext, "No extension found for image!"

    basename = '.'.join([str(hashlib.md5(image_url.encode()).hexdigest()), ext])
    filename = os.path.join(dirname, basename)

    if basename not in existing_images:
        log('Downloading {0} to {1}'.format(image_url, filename))
        response = _urlopen(image_url)
        try:
            with open(filename, 'wb') as f:
                f.write(response.read())
        finally:
            response.close()

    else:
        log('{0} has already been downloaded as {1}'.format(image_url, filename))

    return {
        'filename': filename,
        'basename': basename,
        'was_cached': basename in existing_images
    }


def _setup_download_directory(info):
    dirname = os.path.expanduser(info['download_dir'])
    if not os.path.exists(dirname):
        os.makedirs(dirname)
        log('Created directory for downloaded images at "{0}"'.format(dirname))
    elif info['clear']:
        shutil.rmtree(dirname)
        os.makedirs(dirname)
        log('Cleared downloaded files from "{0}"'.format(dirname))
    else:
        log('Images directory already exists at "{0}"'.format(dirname))


class Post(object):
    def __init__(self, subreddit, title, url):
        self.subreddit = subreddit
        self.title = title
        self.url = url

    def meets_resolution_criteria(self):
        min_resolution = self.subreddit.info['min_resolution']

        if not min_resolution:
            return True

        min_x, min_y = list(map(int, min_resolution.split('x')))

        match = RE_RESOLUTION_IN_TITLE.match(self.title)

        if not match:
            return False

        resolution_x = int(match.group(1))
        resolution_y = int(match.group(2))

        return resolution_x >= min_x and resolution_y >= min_y

    def fetch_image_url(self):
        url = self.url

        ext = _get_extension_from_url(url)

        if ext in IMAGE_EXTENSIONS:
            return url
        elif 'imgur.com' in url:
            return '{0}.jpg'.format(url)
        else:
            return None


class Subreddit(object):
    def __init__(self, info, name, sort='top', limit=25, timeframe='week'):
        self.info = info
        self.name = name
        self.sort = sort
        self.limit = limit
        self.timeframe = timeframe

    def _fetch_posts(self):
        url = 'http://reddit.com/r/{subreddit}/{sort}.json?t={timeframe}&limit={limit}'
        url = url.format(subreddit=self.name,
                         sort=self.sort,
                         timeframe=self.timeframe,
                         limit=self.limit)

        response = _urlopen(url)

        try:
            data = json.loads(response.read())
        finally:
            response.close()

        posts = []
        for child in data['data']['children']:
            data = child['data']
            post = Post(self, data['title'], data['url'])
            posts.append(post)

        return posts

    def fetch_image_urls(self):
        posts = self._fetch_posts()

        image_urls = []
        for post in posts:
            if post.meets_resolution_criteria():
                image_url = post.fetch_image_url()
                if image_url:
                    image_urls.append(image_url)

        log('Found {0} candidate images from {1}'.format(len(image_urls),
                                                         self.name))
        return image_urls

    @classmethod
    def create_from_token(cls, info, token):
        token_parts = token.split(':')

        name = token_parts[0]

        args = ('name', 'sort', 'limit', 'timeframe')
        ddict = {}
        for arg, value in zip(args, token_parts):
            ddict[arg] = value
        return cls(info, **ddict)

    def __repr__(self):
        return '<Subreddit r/{0}>'.format(self.name)


def _read_config_file(info):
    path = os.path.expanduser(CONFIG_PATH)

    if not os.path.exists(path):
        return

    def parse_subreddit_tokens(info):
        tokens = [x.strip() for x in config.get('default', 'subreddits').split(',')]

        if tokens:
            info['subreddit_tokens'] = tokens

    def parse_min_resolution(info):
        try:
            min_resolution = config.get('default', 'min_resolution')
        except configparser.NoOptionError:
            pass
        else:
            info['min_resolution'] = min_resolution

    def parse_download_dir(info):
        try:
            download_dir = config.get('default', 'download_dir')
        except configparser.NoOptionError:
            pass
        else:
            info['download_dir'] = download_dir

    config = configparser.ConfigParser()
    with open(path) as f:
        config.read_file(f)

    if not info.get('subreddit_tokens'):
        parse_subreddit_tokens(info)

    if not info.get('min_resolution'):
        parse_min_resolution(info)

    if not info.get('download_dir'):
        parse_download_dir(info)


def _handle_cli_options(info):
    parser = argparse.ArgumentParser(
        description='Set desktop background image from reddit')
    parser.add_argument('subreddits', metavar='SUBREDDITS', nargs='*',
            help='A list of subreddits')
    parser.add_argument('--min-resolution',
            help='Minimum resolution allowed for image'
                 ' (default: {0})'.format(DEFAULT_MIN_RESOLUTION))
    parser.add_argument('--download-dir',
            help='Directory to download images to'
                 ' (default: {0})'.format(DEFAULT_DOWNLOAD_DIRECTORY))
    parser.add_argument('--clear', action='store_true',
            help='Clear the currently downloaded images')
    parser.add_argument('--verbose', action='store_true',
            help='Prints addition information to stderr')

    args = parser.parse_args()

    set_verbose(args.verbose)

    if args.min_resolution:
        info['min_resolution'] = args.min_resolution

    if args.subreddits:
        info['subreddit_tokens'] = args.subreddits

    info['clear'] = args.clear


def _use_defaults(info):
    if not info.get('subreddit_tokens'):
        # CityPorn seems like a pretty good default
        info['subreddit_tokens'] = ['CityPorn']

    if not info.get('clear'):
        info['clear'] = False

    if not info.get('download_dir'):
        info['download_dir'] = DEFAULT_DOWNLOAD_DIRECTORY


def main():
    # Configuration override chain:
    #   CLI options overrides config file
    #   config file overrides defaults
    info = collections.defaultdict(dict)

    _handle_cli_options(info)
    _read_config_file(info)
    _use_defaults(info)

    _setup_download_directory(info)

    # Collect requested subreddits
    subreddits = []
    for token in info['subreddit_tokens']:
        subreddit = Subreddit.create_from_token(info, token)
        subreddits.append(subreddit)

    # Accumulate images URLs across subreddits
    image_urls = []
    for subreddit in subreddits:
        urls = subreddit.fetch_image_urls()
        image_urls.extend(urls)

    # List which files already have been downloaded
    existing_images = []
    download_dir = os.path.expanduser(info['download_dir'])
    if os.path.exists(download_dir):
        existing_images = [ f for f in os.listdir(download_dir) if os.path.isfile(os.path.join(download_dir, f)) ]

    # Download available images
    current_images = []
    for image_url in image_urls:
        try:
            image_download = _download_image(download_dir, image_url, existing_images)
            current_images.append(image_download['basename'])
        except urllib.error.HTTPError:
            continue  # Try next image...

    # Remove cached files that aren't on the subreddit results anymore
    for file_to_delete in set(existing_images) - set(current_images):
        os.remove(os.path.join(download_dir, file_to_delete))
        log("Removing expired file \"{0}\"".format(file_to_delete))


if __name__ == "__main__":
    main()
